\section{Introduction}\label{introduction}

\begin{frame}{Overview}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  What is Machine Learning?

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Machine Learning is the study of computer algorithms that improve
    automatically through experience.
  \end{itemize}
\item
  How should I go about learning Machine Learning?

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    MOOCs
  \item
    Don't get caught up in the implementations.
  \end{itemize}
\item
  Tools

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    WEKA
  \item
    scikit-learn
  \end{itemize}
\end{itemize}

\end{frame}

\section{Machine Learning}\label{machine-learning}

\begin{frame}{Types}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Supervised Learning

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Supervised Learning is the task of inferring a function from labeled
    training data.
  \end{itemize}
\item
  Unsupervised Learning

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Unsupervised Learning is the tasks of finding hidden structure in
    unlabeled data.
  \end{itemize}
\item
  Reenforcement Learning

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Reenforcement Learning is concerned with how agents ought to take
    actions in an environment as to maximize some notion of cumulative
    reward.
  \item
    Trade off between exploitation and exploration.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Some Boring, but important Definitions.}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Inductive Bias

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The inductive bias of a learning algorithm is the set of assumptions
    that the learner uses to predict outputs given inputs that it has
    not encountered.
  \item
    Occam's Razor assumes that the hypotheses with the fewest
    assumptions should be selected.
  \end{itemize}
\item
  Cross-validation

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The basic idea of Cross-validation to leave out some of the data
    when fitting the model.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Scikit-learn}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Scikit-learn is a set of simple and efficient tools for data mining
  and data analysis.
\item
  Uses Python!!!
\item
  \url{http://scikit-learn.org/}
\end{itemize}

\end{frame}

\section{Supervised Learning}\label{supervised-learning}

\begin{frame}{Decision Trees}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Decision Tree learning is a method for approximating discrete-valued
  target functions, in which the learned function is represented a
  decision tree.
\item
  Maximize Information Gain

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Information Gain measures how well a given attribute separates the
    training examples according to their target classification.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Decision Trees: Example}

\begin{verbatim}
import numpy as np
import pylab as pl
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
# Parameters
# Load data
iris = load_iris()
clf = DecisionTreeClassifier()
X = iris.data[:, [1, 2]]
y = iris.target
clf = clf.fit(X, y)
plotCustom(X, y, [1, 2], clf)`
\end{verbatim}

\end{frame}

\begin{frame}{kNN}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  K-Nearest neighbor algorithm

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    kNN is a example of a instance based learning algorithm.
  \item
    Output is classified by a majority vote of its neighbors, where the
    class that is most common of a instances K neighbors.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{kNN: Example}

\begin{verbatim}
from sklearn import neighbors
import numpy as np
import pylab as pl
from sklearn import cross_validation
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:, [1, 2]]
y = iris.target
clf = neighbors.KNeighborsClassifier(3, 'distance')
plotCustom(X, y, [1,2], clf)
\end{verbatim}

\end{frame}

\begin{frame}{SVM}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Support Vector Machines

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    SVM's are a class of linear classifiers.
  \end{itemize}
\item
  Kernel Trick
\end{itemize}

\end{frame}

\begin{frame}[fragile]{SVM: Example}

\begin{verbatim}
from sklearn import svm
import numpy as np
import pylab as pl
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:, [1, 2]]
y = iris.target
C = 1.0
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C)
rbf_svc.fit(X,y)
plotCustom(X, y, [1,2], rbf_svc)
\end{verbatim}

\end{frame}

\section{Unsupervised Learning}\label{unsupervised-learning}

\begin{frame}{kMeans}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The k-means algorithm clusters data by trying to separate samples into
  n groups of equal variance.
\item
  The name is derived from the representing k clusters by the mean of
  its points.
\item
  K-Means works well with numerical attributes.
\end{itemize}

\end{frame}

\begin{frame}[fragile]{kMeans: Example}

\begin{verbatim}
import numpy as np
import pylab as pl
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:, [1, 2]]
y = iris.target
n_digits = len(np.unique(y))
kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)
kmeans.fit(X)
kmeans_plots(X,y,[1, 2],kmeans)
\end{verbatim}

\end{frame}

\section{Conclusion}\label{conclusion}

\begin{frame}{Resources}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  MOOCS

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \href{https://www.udacity.com/course/ud675}{Udacity}
  \item
    \href{https://www.coursera.org/course/ml}{Coursera}
  \item
    \href{https://weka.waikato.ac.nz/dataminingwithweka}{Data Mining
    with Weka}
  \end{itemize}
\item
  Text

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \href{http://www.cs.cmu.edu/~tom/mlbook.html}{Machine Learning,
    Mitchell}
  \end{itemize}
\end{itemize}

\end{frame}
